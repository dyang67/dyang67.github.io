---
---


@STRING{CVPR = {Proc. IEEE Conf. on Computer Vision and Pattern	Recognition (CVPR)}}
@STRING{ECCV = {Proc. of the European Conf. on Computer Vision (ECCV)}}
@STRING{ICCV = {Proc. of the IEEE International Conf. on Computer Vision (ICCV)}}
@STRING{THREEDV = {Proc. of the International Conf. on 3D Vision (3DV)}}
@STRING{NEURIPS = {Advances in Neural Information Processing Systems (NeurIPS)}}
@STRING{ARXIV = {arXiv.org}}
@STRING{EMNLP = {Proc. of the Empirical Methods in Natural Language Processing (EMNLP)}}

@inproceedings{Dong2022EMNLP,
  author    = {Dong Yang, Peijun Qing, Yang Li, Haonan Lu, Xiaodong Lin},
  title     = {GammaE: Gamma Embeddings for Logical Queries on Knowledge Graphs},
  booktitle   = NEURIPS,
  year      = {2022},
  html = {},
  pdf = {},
  supp = {},
  award = {Oral Presentation},
  code = {},
  img            = {},
}


@inproceedings{Schwarz2022NEURIPS,
  author    = {Dong Yang, Monica Mengqi Li, Hong Fu, Jicong Fan, Zhao Zhang, Howard Leung},
  title     = {Unifying Graph Embedding Features with Graph Convolutional Networks for Skeleton-based Action Recognition},
  booktitle   = NEURIPS,
  year      = {2022},
  pdf = {https://arxiv.org/pdf/2206.07695.pdf},
  supp = {https://arxiv.org/pdf/2206.07695.pdf},
  html = {https://katjaschwarz.github.io/voxgraf/},
  code = {https://github.com/autonomousvision/voxgraf},
  img            = {assets/img/publications/voxgraf.jpg},
}


@inproceedings{Niemeyer2022CVPR,
  author          = {Michael Niemeyer and Jonathan T. Barron and Ben Mildenhall and Mehdi S. M. Sajjadi and Andreas Geiger and Noha Radwan},
  title           = {RegNeRF: Regularizing Neural Radiance Fields for View Synthesis from Sparse Inputs},
  booktitle       = CVPR,
  year            = {2022},
  abstract        = {Neural Radiance Fields (NeRF) have emerged as a powerful representation for the task of novel view synthesis due to their simplicity and state-of-the-art performance. Though NeRF can produce photorealistic renderings of unseen viewpoints when many input views are available, its performance drops significantly when this number is reduced. We observe that the majority of artifacts in sparse input scenarios are caused by errors in the estimated scene geometry, and by divergent behavior at the start of training. We address this by regularizing the geometry and appearance of patches rendered from unobserved viewpoints, and annealing the ray sampling space during training. We additionally use a normalizing flow model to regularize the color of unobserved viewpoints. Our model outperforms not only other methods that optimize over a single scene, but in many cases also conditional models that are extensively pre-trained on large multi-view datasets.},
  img            = {assets/img/publications/regnerf.jpg},
  html = {https://m-niemeyer.github.io/regnerf},
  pdf = {https://drive.google.com/file/d/1S_NnmhypZjyMfwqcHg-YbWSSYNWdqqlo/view?usp=sharing},
  supp = {https://drive.google.com/file/d/15ip8Fvfxp6rNRfBnbJEnFCjIJeFMH4CE/view?usp=sharing},
  video = {https://youtu.be/QyyyvA4-Kwc},
  code = {https://github.com/google-research/google-research/tree/master/regnerf},
  selected       = {true},
  award = {Oral Presentation},
  poster ={https://drive.google.com/file/d/1kYknB2Ap3I3avstmPxAa9IiW8m85AZEF/view?usp=sharing},
}


@INPROCEEDINGS{Peng2021NEURIPS,
  author = {Songyou Peng and Chiyu Max Jiang and Yiyi Liao and Michael Niemeyer and Marc Pollefeys and Andreas Geiger},
  title = {Shape As Points: A Differentiable Poisson Solver},
  booktitle = NEURIPS,
  abstract = {In recent years, neural implicit representations gained popularity in 3D reconstruction due to their expressiveness and flexibility. However, the implicit nature of neural implicit representations results in slow inference times and requires careful initialization. In this paper, we revisit the classic yet ubiquitous point cloud representation and introduce a differentiable point-to-mesh layer using a differentiable formulation of Poisson Surface Reconstruction (PSR) which allows for a GPU-accelerated fast solution of the indicator function given an oriented point cloud. The differentiable PSR layer allows us to efficiently and differentiably bridge the explicit 3D point representation with the 3D mesh via the implicit indicator field, enabling end-to-end optimization of surface reconstruction metrics such as Chamfer distance. This duality between points and meshes hence allows us to represent shapes as oriented point clouds, which are explicit, lightweight and expressive. Compared to neural implicit representations, our Shape-As-Points (SAP) model is more interpretable, lightweight, and accelerates inference time by one order of magnitude. Compared to other explicit representations such as points, patches, and meshes, SAP produces topology-agnostic, watertight manifold surfaces. We demonstrate the effectiveness of SAP on the task of surface reconstruction from unoriented point clouds and learning-based reconstruction.},
  year = {2021},
  img            = {assets/img/publications/sap.jpg},
  award = {Oral Presentation},
  html = {https://pengsongyou.github.io/sap},
  code = {https://github.com/autonomousvision/shape_as_points},
  video = {https://youtu.be/FL8LMk_qWb4},
  poster = {https://pengsongyou.github.io/media/sap/sap_poster.pdf},
  pdf = {https://arxiv.org/abs/2106.03452},
} 

@InProceedings{Schwarz2020NEURIPS,
  author         = {Katja Schwarz and Yiyi Liao and Michael Niemeyer and Andreas Geiger},
  title          = {GRAF: Generative Radiance Fields for 3D-Aware Image Synthesis},
  booktitle      = NEURIPS,
  year           = {2020},
  abstract       = {While 2D generative adversarial networks have enabled high-resolution image synthesis, they largely lack an understanding of the 3D world and the image formation process. Thus, they do not provide precise control over camera viewpoint or object pose. To address this problem, several recent approaches leverage intermediate voxel-based representations in combination with differentiable rendering. However, existing methods either produce low image resolution or fall short in disentangling camera and scene properties, eg, the object identity may vary with the viewpoint. In this paper, we propose a generative model for radiance fields which have recently proven successful for novel view synthesis of a single scene. In contrast to voxel-based representations, radiance fields are not confined to a coarse discretization of the 3D space, yet allow for disentangling camera and scene properties while degrading gracefully in the presence of reconstruction ambiguity. By introducing a multi-scale patch-based discriminator, we demonstrate synthesis of high-resolution images while training our model from unposed 2D images alone. We systematically analyze our approach on several challenging synthetic and real-world datasets. Our experiments reveal that radiance fields are a powerful representation for generative image synthesis, leading to 3D consistent models that render with high fidelity.},
  cvlibs_month   = {12},
  code = {https://github.com/autonomousvision/graf},
  cvlibs_youtube = {akQf7WaCOHo},
  groups         = {cvlibs},
  img            = {assets/img/publications/graf.jpg},
  pdf = {http://www.cvlibs.net/publications/Schwarz2020NEURIPS.pdf},
  supp = {http://www.cvlibs.net/publications/Schwarz2020NEURIPS_supplementary.pdf},
  video = {http://www.youtube.com/watch?v=akQf7WaCOHo&vq=hd1080&autoplay=1},
  blog = {https://autonomousvision.github.io/graf/},
  selected       = {true},
  html = {https://ps.is.mpg.de/publications/schwarz2020neurips},

}

@InProceedings{Niemeyer2021CVPR,
  author         = {Michael Niemeyer and Andreas Geiger},
  title          = {GIRAFFE: Representing Scenes as Compositional Generative Neural Feature Fields},
  booktitle      = CVPR,
  year           = {2021},
  abstract       = {Deep generative models allow for photorealistic image synthesis at high resolutions. But for many applications, this is not enough: content creation also needs to be controllable. While several recent works investigate how to disentangle underlying factors of variation in the data, most of them operate in 2D and hence ignore that our world is three-dimensional. Further, only few works consider the compositional nature of scenes. Our key hypothesis is that incorporating a compositional 3D scene representation into the generative model leads to more controllable image synthesis. Representing scenes as compositional generative neural feature fields allows us to disentangle one or multiple objects from the background as well as individual objects' shapes and appearances while learning from unstructured and unposed image collections without any additional supervision. Combining this scene representation with a neural rendering pipeline yields a fast and realistic image synthesis model. As evidenced by our experiments, our model is able to disentangle individual objects and allows for translating and rotating them in the scene as well as changing the camera pose.},
  html           = {https://m-niemeyer.github.io/project-pages/giraffe/index.html},
  img            = {assets/img/publications/giraffe.jpg},
  pdf = {http://www.cvlibs.net/publications/Niemeyer2021CVPR.pdf},
  supp = {http://www.cvlibs.net/publications/Niemeyer2021CVPR_supplementary.pdf},
  poster = {http://www.cvlibs.net/publications/Niemeyer2021CVPR_poster.pdf},
  video = {http://www.youtube.com/watch?v=fIaDXC-qRSg&vq=hd1080&autoplay=1},  
  code = {https://github.com/autonomousvision/giraffe},
  award = {Oral Presentation, Best Paper Award}
}


